# Local AI Configuration (add to your .env.local)

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_ENABLED=true

# Local Model Preferences
LOCAL_EMAIL_MODEL=mistral:7b-instruct
LOCAL_CALENDAR_MODEL=llama3.1:8b
LOCAL_QUICK_MODEL=phi3:mini
LOCAL_CODE_MODEL=codellama:7b

# AI Router Configuration
AI_ROUTER_URL=http://localhost:3001
AI_PREFER_LOCAL=true
AI_FALLBACK_ENABLED=true

# Feature Flags (enable local AI)
VITE_FEATURE_LOCAL_AI_ROUTING=true
VITE_FEATURE_COST_TRACKING=true
